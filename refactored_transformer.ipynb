{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c6f686-6a61-455c-89da-96fd75037e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ttfstudentTest2_37.pth\n",
      "Error in main execution: Error(s) in loading state_dict for TimeSeriesTransformer:\n",
      "\tMissing key(s) in state_dict: \"encoder_layers.0.0.qkv_projection.weight\", \"encoder_layers.0.0.qkv_projection.bias\", \"encoder_layers.0.0.output_projection.weight\", \"encoder_layers.0.0.output_projection.bias\", \"encoder_layers.0.0.norm.weight\", \"encoder_layers.0.0.norm.bias\", \"encoder_layers.0.1.norm.weight\", \"encoder_layers.0.1.norm.bias\", \"encoder_layers.0.1.ff.0.weight\", \"encoder_layers.0.1.ff.0.bias\", \"encoder_layers.0.1.ff.3.weight\", \"encoder_layers.0.1.ff.3.bias\", \"encoder_layers.1.0.qkv_projection.weight\", \"encoder_layers.1.0.qkv_projection.bias\", \"encoder_layers.1.0.output_projection.weight\", \"encoder_layers.1.0.output_projection.bias\", \"encoder_layers.1.0.norm.weight\", \"encoder_layers.1.0.norm.bias\", \"encoder_layers.1.1.norm.weight\", \"encoder_layers.1.1.norm.bias\", \"encoder_layers.1.1.ff.0.weight\", \"encoder_layers.1.1.ff.0.bias\", \"encoder_layers.1.1.ff.3.weight\", \"encoder_layers.1.1.ff.3.bias\", \"final_norm.norm.weight\", \"final_norm.norm.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"encoder.0.attn.qkv.weight\", \"encoder.0.attn.proj.weight\", \"encoder.0.attn.ln.weight\", \"encoder.0.attn.ln.bias\", \"encoder.0.ff.ln.weight\", \"encoder.0.ff.ln.bias\", \"encoder.0.ff.ff.0.weight\", \"encoder.0.ff.ff.0.bias\", \"encoder.0.ff.ff.2.weight\", \"encoder.0.ff.ff.2.bias\", \"encoder.1.attn.qkv.weight\", \"encoder.1.attn.proj.weight\", \"encoder.1.attn.ln.weight\", \"encoder.1.attn.ln.bias\", \"encoder.1.ff.ln.weight\", \"encoder.1.ff.ln.bias\", \"encoder.1.ff.ff.0.weight\", \"encoder.1.ff.ff.0.bias\", \"encoder.1.ff.ff.2.weight\", \"encoder.1.ff.ff.2.bias\", \"temporal_norm.weight\", \"temporal_norm.bias\". \n",
      "Please verify that AI Edge Torch is properly installed and compatible with your environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Import AI Edge Torch libraries - adjusted for actual package structure\n",
    "from ai_edge_torch._convert import converter\n",
    "# Let's import other modules as needed and check if they exist\n",
    "try:\n",
    "    from ai_edge_torch.generative.layers import normalization\n",
    "    HAS_NORMALIZATION = True\n",
    "except ImportError:\n",
    "    HAS_NORMALIZATION = False\n",
    "    \n",
    "try:\n",
    "    from ai_edge_torch.generative.layers import scaled_dot_product_attention\n",
    "    HAS_SDPA = True\n",
    "except ImportError:\n",
    "    HAS_SDPA = False\n",
    "\n",
    "# Set paths\n",
    "WEIGHTS_PATH = \"/path/to/model_weights.pth\"  # Set this to your model weights path\n",
    "OUTPUT_PATH = \"time_series_transformer.tflite\"\n",
    "\n",
    "# Define core building blocks\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5, enable_hlfb: bool = True):\n",
    "        super().__init__()\n",
    "        self.enable_hlfb = enable_hlfb\n",
    "        # Use AI Edge Torch normalization if available, otherwise use PyTorch\n",
    "        if HAS_NORMALIZATION:\n",
    "            self.norm = normalization.LayerNorm(\n",
    "                dim=dim,\n",
    "                eps=eps,\n",
    "                enable_hlfb=enable_hlfb\n",
    "            )\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(dim, eps=eps)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(x)\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0, enable_hlfb: bool = True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.enable_hlfb = enable_hlfb and HAS_SDPA\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(embed_dim, 3 * embed_dim, bias=True)\n",
    "        self.output_projection = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        B, T, _ = x.shape\n",
    "        qkv = self.qkv_projection(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, T, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each has shape [B, num_heads, T, head_dim]\n",
    "        \n",
    "        # Use scaled_dot_product_attention\n",
    "        if self.enable_hlfb:\n",
    "            # For better on-device performance, use the HLFB version if available\n",
    "            attn = scaled_dot_product_attention.scaled_dot_product_attention_with_hlfb(\n",
    "                q, k, v, self.head_dim\n",
    "            )\n",
    "        else:\n",
    "            # Standard implementation\n",
    "            attn = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, dropout_p=0.0, is_causal=False\n",
    "            )\n",
    "        \n",
    "        # Reshape back\n",
    "        attn = attn.transpose(1, 2).reshape(B, T, -1)\n",
    "        output = self.output_projection(attn)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return residual + output\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0, \n",
    "                 activation: str = \"relu\"):\n",
    "        super().__init__()\n",
    "        # Implement a more standard PyTorch feed-forward network\n",
    "        if activation == \"relu\":\n",
    "            act_fn = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            act_fn = nn.GELU()\n",
    "        else:  # Default to ReLU\n",
    "            act_fn = nn.ReLU()\n",
    "            \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            act_fn,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.ff(self.norm(x))\n",
    "\n",
    "# Complete transformer model implementation\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_frames: int = 128,\n",
    "                 input_channels: int = 4,\n",
    "                 num_classes: int = 1,\n",
    "                 embed_dim: int = 32,\n",
    "                 num_heads: int = 4,\n",
    "                 num_layers: int = 2,\n",
    "                 ff_mult: int = 2,\n",
    "                 activation: str = \"relu\",\n",
    "                 dropout: float = 0.1,\n",
    "                 norm_first: bool = True,\n",
    "                 enable_hlfb: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        # Input projection with 1D convolution\n",
    "        pad = (8 - 1) // 2  # Same padding for kernel=8\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, embed_dim, kernel_size=8, stride=1, padding=pad),\n",
    "            nn.BatchNorm1d(embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Build transformer layers\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            attention_block = SelfAttentionBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                enable_hlfb=enable_hlfb\n",
    "            )\n",
    "            \n",
    "            ff_block = FeedForwardBlock(\n",
    "                dim=embed_dim,\n",
    "                hidden_dim=embed_dim * ff_mult,\n",
    "                dropout=dropout,\n",
    "                activation=activation\n",
    "            )\n",
    "            \n",
    "            self.encoder_layers.append(nn.ModuleList([attention_block, ff_block]))\n",
    "        \n",
    "        # Output layers\n",
    "        self.final_norm = CustomLayerNorm(embed_dim, enable_hlfb=enable_hlfb)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [batch_size, time_steps, channels]\n",
    "        # Rearrange for Conv1D\n",
    "        x = x.transpose(1, 2)  # [batch_size, channels, time_steps]\n",
    "        x = self.input_proj(x)\n",
    "        x = x.transpose(1, 2)  # [batch_size, time_steps, embed_dim]\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        for attn, ff in self.encoder_layers:\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "        \n",
    "        # Final norm and pooling\n",
    "        x = self.final_norm(x)\n",
    "        x = x.mean(dim=1)  # Global temporal pooling\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# Model conversion utilities\n",
    "def convert_model(model, sample_input, output_path, quantize=True):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch model to TFLite format.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to convert\n",
    "        sample_input: A sample input tensor\n",
    "        output_path: Path where to save the TFLite model\n",
    "        quantize: Whether to quantize the model (default: True)\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create signature instance\n",
    "    if isinstance(sample_input, tuple):\n",
    "        signature = converter.signature_module.Signature(\n",
    "            name=\"serving_default\",\n",
    "            module=model,\n",
    "            sample_args=sample_input,\n",
    "            sample_kwargs={}\n",
    "        )\n",
    "    else:\n",
    "        signature = converter.signature_module.Signature(\n",
    "            name=\"serving_default\",\n",
    "            module=model,\n",
    "            sample_args=(sample_input,),\n",
    "            sample_kwargs={}\n",
    "        )\n",
    "    \n",
    "    # Convert using conversion module\n",
    "    edge_model = converter.conversion.convert_signatures(\n",
    "        [signature],\n",
    "        strict_export=False\n",
    "    )\n",
    "    \n",
    "    # Export the model\n",
    "    edge_model.export(output_path)\n",
    "    \n",
    "    print(f\"Model successfully converted and saved to {output_path}\")\n",
    "    return edge_model\n",
    "\n",
    "# Create and initialize model\n",
    "def create_model():\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_frames=128,\n",
    "        input_channels=3,\n",
    "        num_classes=1,\n",
    "        embed_dim=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        ff_mult=2,\n",
    "        activation=\"relu\",\n",
    "        dropout=0.1,\n",
    "        norm_first=True,\n",
    "        enable_hlfb=True\n",
    "    )\n",
    "    \n",
    "    # Load weights if they exist\n",
    "    WEIGHTS_PATH='ttfstudentTest2_37.pth'\n",
    "    if os.path.exists(WEIGHTS_PATH):\n",
    "        print(f\"Loading weights from {WEIGHTS_PATH}\")\n",
    "        weights = torch.load(WEIGHTS_PATH, map_location='cpu')\n",
    "        model.load_state_dict(weights)\n",
    "    else:\n",
    "        print(f\"Warning: Weights file not found at {WEIGHTS_PATH}. Using random initialization.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model with a random input\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    sample_input = torch.randn(1, 128, 4)  # [batch_size, time_steps, channels]\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    return sample_input, output\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    # Create and initialize model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Test the model\n",
    "    sample_input, _ = test_model(model)\n",
    "    \n",
    "    # Convert the model to TFLite\n",
    "    try:\n",
    "        edge_model = convert_model(model, sample_input, OUTPUT_PATH, quantize=True)\n",
    "        print(\"Model conversion process completed!\")\n",
    "        return edge_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model conversion: {e}\")\n",
    "        print(\"Trying alternative conversion approach...\")\n",
    "        \n",
    "        # Alternative approach using direct TorchScript\n",
    "        try:\n",
    "            # Try using torch.jit.trace directly\n",
    "            traced_model = torch.jit.trace(model, sample_input)\n",
    "            traced_model.save(\"model.pt\")\n",
    "            print(\"Model saved as TorchScript model.pt\")\n",
    "            \n",
    "            print(\"You'll need to use the TFLite Converter tool to convert from TorchScript to TFLite.\")\n",
    "            print(\"Example command:\")\n",
    "            print(\"tflite_convert --saved_model_dir=./model.pt --output_file=model.tflite\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Error during TorchScript conversion: {e2}\")\n",
    "            print(\"Please check the AI Edge Torch installation and documentation.\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    edge_model = main()\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {e}\")\n",
    "    print(\"Please verify that AI Edge Torch is properly installed and compatible with your environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0100d11-c258-401f-becf-a425d382b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "class TransModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series Transformer for fall detection compatible with existing checkpoint\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_layers=2,\n",
    "                 norm_first=True,\n",
    "                 embed_dim=32,\n",
    "                 activation='relu',\n",
    "                 acc_coords=3,  # Must be 3 to match checkpoint\n",
    "                 num_classes=1,\n",
    "                 acc_frames=128,\n",
    "                 mocap_frames=128,\n",
    "                 num_heads=4,\n",
    "                 ff_dim=64,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.acc_frames = acc_frames\n",
    "        self.acc_coords = acc_coords\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Input projection - Conv1D + BatchNorm (exact match to checkpoint)\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Conv1d(acc_coords, embed_dim, kernel_size=8, stride=1, padding=3),\n",
    "            nn.BatchNorm1d(embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Build encoder layers (must match checkpoint structure exactly)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            # Each layer has an attention block and a feedforward block\n",
    "            attn_block = SelfAttentionBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            ff_block = FeedForwardBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            self.encoder.append(nn.ModuleList([attn_block, ff_block]))\n",
    "        \n",
    "        # Final normalization (named to match checkpoint)\n",
    "        self.temporal_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Output head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for fall detection\"\"\"\n",
    "        # Handle different input formats\n",
    "        if isinstance(x, dict):\n",
    "            if 'accelerometer' in x:\n",
    "                x = x['accelerometer']\n",
    "            else:\n",
    "                x = next(iter(x.values()))\n",
    "        \n",
    "        # Check input shape and format correctly\n",
    "        if x.dim() == 3:\n",
    "            if x.shape[1] == self.acc_coords and x.shape[2] == self.acc_frames:\n",
    "                # Already in format [B, C, T]\n",
    "                pass\n",
    "            elif x.shape[1] == self.acc_frames and x.shape[2] == self.acc_coords:\n",
    "                # Convert from [B, T, C] to [B, C, T]\n",
    "                x = x.transpose(1, 2)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected input shape (B, {self.acc_coords}, {self.acc_frames}) or \"\n",
    "                               f\"(B, {self.acc_frames}, {self.acc_coords}), got {x.shape}\")\n",
    "        \n",
    "        # Replace NaN values with zeros for stability\n",
    "        x = torch.nan_to_num(x, nan=0.0)\n",
    "        \n",
    "        # Apply convolutional projection\n",
    "        x = self.in_proj(x)  # [B, C, T]\n",
    "        \n",
    "        # Transpose to sequence format for transformer\n",
    "        x = x.transpose(1, 2)  # [B, T, C]\n",
    "        \n",
    "        # Apply transformer encoder layers\n",
    "        for attn, ff in self.encoder:\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x = self.temporal_norm(x)\n",
    "        \n",
    "        # Global average pooling over sequence dimension\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Apply final classification head\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention block matching checkpoint structure\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Single QKV projection as in the checkpoint\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Store parameters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply residual connection pattern\n",
    "        residual = x\n",
    "        \n",
    "        # Layer normalization\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # Get shapes\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, H, N, D]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Create causal mask (lower triangular)\n",
    "        mask = torch.triu(torch.ones(N, N, device=x.device), diagonal=1) * -1e9\n",
    "        attn = attn + mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        # Apply output projection\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"Feed-forward block matching checkpoint structure\"\"\"\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply residual connection pattern\n",
    "        residual = x\n",
    "        \n",
    "        # Layer normalization\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        x = self.ff(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = x + residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d9873f-c441-4d38-b518-b340445877fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ttfstudentTest2_37.pth\n",
      "Model output shape: torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Use JAX lowering: aten._native_batch_norm_legit_no_training\n",
      "INFO:root:Use JAX lowering: aten.permute\n",
      "INFO:root:Use JAX lowering: aten.view\n",
      "INFO:root:Use JAX lowering: aten.mm\n",
      "INFO:root:Use JAX lowering: aten.select\n",
      "INFO:root:Use JAX lowering: aten.mul.Scalar\n",
      "INFO:root:Use JAX lowering: aten.expand\n",
      "INFO:root:Use JAX lowering: aten.clone.default\n",
      "INFO:root:Use JAX lowering: aten.bmm\n",
      "INFO:root:Use JAX lowering: aten._softmax\n",
      "INFO:root:Use JAX lowering: aten.add.Tensor\n",
      "INFO:root:Use JAX lowering: aten.mul.Tensor\n",
      "INFO:root:Use JAX lowering: aten.relu\n",
      "INFO:root:Use JAX lowering: aten.mean\n",
      "I0000 00:00:1745249525.902734   37805 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "INFO:absl:Function `inner` contains input name(s) resource with unsupported characters which will be renamed to xlacallmodule_readvariableop_30_resource in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0ciygy00/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0ciygy00/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted and saved to time_series_transformer.tflite\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1745249527.072029   37805 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1745249527.072117   37805 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-04-21 10:32:07.073339: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp0ciygy00\n",
      "2025-04-21 10:32:07.073980: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-04-21 10:32:07.073991: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp0ciygy00\n",
      "I0000 00:00:1745249527.080699   37805 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-04-21 10:32:07.081536: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-04-21 10:32:07.135812: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp0ciygy00\n",
      "2025-04-21 10:32:07.144849: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 71515 microseconds.\n",
      "2025-04-21 10:32:07.182469: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-21 10:32:07.408235: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:4061] Estimated count of arithmetic ops: 12.779 M  ops, equivalently 6.390 M  MACs\n"
     ]
    }
   ],
   "source": [
    "# AI Edge Torch: Transformer Model Conversion to TFLite\n",
    "# Matching exactly the structure of the existing weights\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import logging\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Import AI Edge Torch libraries\n",
    "try:\n",
    "    from ai_edge_torch._convert import converter\n",
    "    HAS_AI_EDGE_TORCH = True\n",
    "except ImportError:\n",
    "    print(\"Warning: ai_edge_torch not found, conversion to TFLite won't be possible\")\n",
    "    HAS_AI_EDGE_TORCH = False\n",
    "\n",
    "# Set paths\n",
    "WEIGHTS_PATH = \"ttfstudentTest2_37.pth\"  # Set to your model weights path\n",
    "OUTPUT_PATH = \"time_series_transformer.tflite\"\n",
    "\n",
    "# This model class exactly matches the structure in the checkpoint file\n",
    "class TransModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 acc_frames: int = 128,\n",
    "                 acc_coords: int = 3,  # Must be 3 to match weights\n",
    "                 num_classes: int = 1,\n",
    "                 embed_dim: int = 32,\n",
    "                 num_heads: int = 4,\n",
    "                 num_layers: int = 2,\n",
    "                 ff_mult: int = 2,\n",
    "                 activation: str = \"relu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input projection with 1D convolution - named exactly like in checkpoint\n",
    "        pad = (8 - 1) // 2  # 3 for k=8\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Conv1d(acc_coords, embed_dim, kernel_size=8, stride=1, padding=pad),\n",
    "            nn.BatchNorm1d(embed_dim)\n",
    "        )\n",
    "\n",
    "        # Encoder layers structure - matches checkpoint exactly\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            # Create SelfAttentionBlock with exact name matching\n",
    "            attn = nn.Module()\n",
    "            attn.ln = nn.LayerNorm(embed_dim)\n",
    "            attn.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "            attn.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "            \n",
    "            # Create FeedForwardBlock with exact name matching\n",
    "            ff = nn.Module()\n",
    "            ff.ln = nn.LayerNorm(embed_dim)\n",
    "            ff.ff = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * ff_mult),\n",
    "                nn.ReLU() if activation == \"relu\" else nn.GELU(),\n",
    "                nn.Linear(embed_dim * ff_mult, embed_dim)\n",
    "            )\n",
    "            \n",
    "            # Add both blocks to encoder as in checkpoint\n",
    "            layer = nn.Module()\n",
    "            layer.attn = attn\n",
    "            layer.ff = ff\n",
    "            self.encoder.append(layer)\n",
    "            \n",
    "        # Normalization layer - named exactly as in checkpoint\n",
    "        self.temporal_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, acc_data: torch.Tensor) -> torch.Tensor:\n",
    "        # Handle different input formats\n",
    "        if isinstance(acc_data, dict):\n",
    "            if 'accelerometer' in acc_data:\n",
    "                acc_data = acc_data['accelerometer']\n",
    "            else:\n",
    "                acc_data = next(iter(acc_data.values()))\n",
    "                \n",
    "        # Make sure input is in the right format [B, C, T] for Conv1D\n",
    "        if acc_data.dim() == 3 and acc_data.shape[1] != 3:\n",
    "            acc_data = acc_data.transpose(1, 2)\n",
    "        \n",
    "        # Apply input projection\n",
    "        x = self.input_proj(acc_data)  # B × C × T\n",
    "        x = rearrange(x, 'b c t -> b t c')  # B × T × C\n",
    "\n",
    "        # Forward through transformer blocks\n",
    "        for layer in self.encoder:\n",
    "            # Self-attention block\n",
    "            residual = x\n",
    "            x_norm = layer.attn.ln(x)\n",
    "            \n",
    "            # Get QKV\n",
    "            B, T, C = x_norm.shape\n",
    "            head_dim = C // 4  # Hardcoded for 4 heads\n",
    "            \n",
    "            qkv = layer.attn.qkv(x_norm)\n",
    "            qkv = qkv.reshape(B, T, 3, 4, head_dim)\n",
    "            qkv = qkv.permute(2, 0, 3, 1, 4)  # 3, B, H, T, D\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            # Calculate attention\n",
    "            attn = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n",
    "            attn = attn.transpose(1, 2).reshape(B, T, C)\n",
    "            attn = layer.attn.proj(attn)\n",
    "            x = residual + attn\n",
    "            \n",
    "            # Feed forward block\n",
    "            residual = x\n",
    "            x = layer.ff.ln(x)\n",
    "            x = layer.ff.ff(x)\n",
    "            x = residual + x\n",
    "\n",
    "        # Final norm and pooling\n",
    "        x = self.temporal_norm(x)\n",
    "        x = x.mean(dim=1)  # Global temporal pooling\n",
    "        \n",
    "        # Output head\n",
    "        return self.head(x)\n",
    "\n",
    "# Model conversion utility\n",
    "def convert_model(model, sample_input, output_path, quantize=True):\n",
    "    \"\"\"Convert a PyTorch model to TFLite format.\"\"\"\n",
    "    if not HAS_AI_EDGE_TORCH:\n",
    "        print(\"AI Edge Torch not available. Model conversion skipped.\")\n",
    "        return None\n",
    "        \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Create signature instance\n",
    "        if isinstance(sample_input, tuple):\n",
    "            signature = converter.signature_module.Signature(\n",
    "                name=\"serving_default\",\n",
    "                module=model,\n",
    "                sample_args=sample_input,\n",
    "                sample_kwargs={}\n",
    "            )\n",
    "        else:\n",
    "            signature = converter.signature_module.Signature(\n",
    "                name=\"serving_default\",\n",
    "                module=model,\n",
    "                sample_args=(sample_input,),\n",
    "                sample_kwargs={}\n",
    "            )\n",
    "        \n",
    "        # Convert using conversion module\n",
    "        edge_model = converter.conversion.convert_signatures(\n",
    "            [signature],\n",
    "            strict_export=False\n",
    "        )\n",
    "        \n",
    "        # Export the model\n",
    "        edge_model.export(output_path)\n",
    "        \n",
    "        print(f\"Model successfully converted and saved to {output_path}\")\n",
    "        return edge_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model conversion: {e}\")\n",
    "        print(\"Trying alternative conversion approach...\")\n",
    "        \n",
    "        # Alternative approach using direct TorchScript\n",
    "        try:\n",
    "            # Try using torch.jit.trace directly\n",
    "            traced_model = torch.jit.trace(model, sample_input)\n",
    "            traced_model.save(\"model.pt\")\n",
    "            print(\"Model saved as TorchScript model.pt\")\n",
    "            \n",
    "            print(\"You'll need to use the TFLite Converter tool to convert from TorchScript to TFLite.\")\n",
    "            print(\"Example command:\")\n",
    "            print(\"tflite_convert --saved_model_dir=./model.pt --output_file=model.tflite\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Error during TorchScript conversion: {e2}\")\n",
    "            \n",
    "        return None\n",
    "\n",
    "# Main execution code\n",
    "def main():\n",
    "    # Create model with exact structure to match checkpoint\n",
    "    model = TransModel(\n",
    "        acc_frames=128,\n",
    "        acc_coords=3,  # Must be 3 to match weights\n",
    "        num_classes=1,\n",
    "        embed_dim=32,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        ff_mult=2,\n",
    "        activation=\"relu\"\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    if os.path.exists(WEIGHTS_PATH):\n",
    "        print(f\"Loading weights from {WEIGHTS_PATH}\")\n",
    "        checkpoint = torch.load(WEIGHTS_PATH, map_location='cpu')\n",
    "        # Handle potential \"model_state_dict\" wrapper\n",
    "        if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "            checkpoint = checkpoint[\"model_state_dict\"]\n",
    "        model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        print(f\"Warning: Weights file not found at {WEIGHTS_PATH}\")\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Test with random input\n",
    "    sample_input = torch.randn(1, 3, 128)  # [batch_size, channels, time_steps]\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input)\n",
    "        print(f\"Model output shape: {output.shape}\")\n",
    "    \n",
    "    # Convert model to TFLite\n",
    "    converted_model = convert_model(model, sample_input, OUTPUT_PATH)\n",
    "    \n",
    "    return model, converted_model\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    model, converted_model = main()\n",
    "    print(\"Process completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aee303-9045-4447-9c33-52713a09c999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_EDGE_TORCH",
   "language": "python",
   "name": "ai_edge_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
