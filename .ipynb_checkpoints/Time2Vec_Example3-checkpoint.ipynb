{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae52a3c9-a7b8-4e10-9d5c-b2c35b16b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai-edge-torch in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (2.5.0)\n",
      "Requirement already satisfied: torch-xla>=2.4.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (2.5.0)\n",
      "Requirement already satisfied: safetensors in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (0.4.5)\n",
      "Requirement already satisfied: scipy in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (1.14.1)\n",
      "Requirement already satisfied: tf-nightly>=2.18.0.dev20240722 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (2.19.0.dev20241018)\n",
      "Requirement already satisfied: numpy in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (2.1.2)\n",
      "Requirement already satisfied: ai-edge-quantizer-nightly==0.0.1.dev20240718 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (0.0.1.dev20240718)\n",
      "Requirement already satisfied: tabulate in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-torch) (0.9.0)\n",
      "Requirement already satisfied: immutabledict in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from ai-edge-quantizer-nightly==0.0.1.dev20240718->ai-edge-torch) (4.2.0)\n",
      "Requirement already satisfied: tb-nightly~=2.19.0.a in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.19.0a20241018)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.37.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (1.6.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.12.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (5.28.2)\n",
      "Requirement already satisfied: packaging in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (24.1)\n",
      "Requirement already satisfied: setuptools in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (59.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.32.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (4.12.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (24.3.25)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly>=3.6.0.dev in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.6.0.dev2024101603)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (1.67.0)\n",
      "Requirement already satisfied: filelock in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (3.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (2.21.5)\n",
      "Requirement already satisfied: jinja2 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (10.3.5.147)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch>=2.4.0->ai-edge-torch) (3.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->ai-edge-torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from torch-xla>=2.4.0->ai-edge-torch) (6.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from astunparse>=1.6.0->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.44.0)\n",
      "Requirement already satisfied: rich in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (13.9.2)\n",
      "Requirement already satisfied: namex in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from requests<3,>=2.21.0->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from requests<3,>=2.21.0->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from requests<3,>=2.21.0->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from requests<3,>=2.21.0->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.2.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tb-nightly~=2.19.0.a->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tb-nightly~=2.19.0.a->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from tb-nightly~=2.19.0.a->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->ai-edge-torch) (3.0.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.6.0.dev->tf-nightly>=2.18.0.dev20240722->ai-edge-torch) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ai-edge-torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07516f23-aa73-4af3-b4d1-1f1bca305ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Model Definition (time2VecStudent.py)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Time2Vec example. This version returns an embedding of shape (N, out_channels).\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=8):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lin_weight = nn.Parameter(torch.randn(1))\n",
    "        self.lin_bias   = nn.Parameter(torch.randn(1))\n",
    "        if out_channels > 1:\n",
    "            self.per_weight = nn.Parameter(torch.randn(out_channels - 1))\n",
    "            self.per_bias   = nn.Parameter(torch.randn(out_channels - 1))\n",
    "        else:\n",
    "            self.per_weight = None\n",
    "            self.per_bias   = None\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t => shape (N,1)\n",
    "        t_lin = self.lin_weight * t + self.lin_bias\n",
    "        if self.per_weight is not None:\n",
    "            alpha = self.per_weight.unsqueeze(0)\n",
    "            beta  = self.per_bias.unsqueeze(0)\n",
    "            t_per = torch.sin(alpha * t + beta)\n",
    "            return torch.cat([t_lin, t_per], dim=-1)\n",
    "        else:\n",
    "            return t_lin\n",
    "\n",
    "class FallTime2VecTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer-based model that takes a (B, T, 3) accelerometer input\n",
    "    (x, y, z inertial data) plus an optional mask (B, T) and time (B, T).\n",
    "    It creates an 8-dim Time2Vec embedding, concatenates it with the 3-channel data\n",
    "    (total feat = 11), passes it through a TransformerEncoder, and outputs (B, num_classes).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 feat_dim=11,      # 3 (accel) + 8 (time2vec) = 11\n",
    "                 d_model=64,\n",
    "                 nhead=4,\n",
    "                 num_layers=2,\n",
    "                 num_classes=2,\n",
    "                 time2vec_dim=8,   # 8-D Time2Vec embedding\n",
    "                 dropout=0.1,\n",
    "                 dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.time2vec_dim = time2vec_dim\n",
    "        self.dropout = dropout\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        # 1) Time2Vec for the time axis\n",
    "        self.time2vec = Time2Vec(out_channels=time2vec_dim)\n",
    "\n",
    "        # 2) Input projection from feat_dim -> d_model\n",
    "        self.input_proj = nn.Linear(feat_dim, d_model)\n",
    "\n",
    "        # 3) Transformer Encoder\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # expecting input (B, T, d_model)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # 4) Output layer for classification\n",
    "        self.fc = nn.Linear(d_model, self.num_classes)\n",
    "\n",
    "    def forward(self, accel_seq, accel_mask=None, accel_time=None):\n",
    "        \"\"\"\n",
    "        accel_seq: shape (B, T, 3) (accelerometer inertial data: x, y, z)\n",
    "        accel_mask: shape (B, T) bool, where True indicates padding\n",
    "        accel_time: shape (B, T) or None; if None, dummy time indices are created\n",
    "        \"\"\"\n",
    "        B, T, C = accel_seq.shape  # Expect C=3\n",
    "\n",
    "        # 1) Create dummy time if not provided\n",
    "        if accel_time is None:\n",
    "            time_idx = torch.arange(T, device=accel_seq.device).unsqueeze(0).expand(B, T).float()\n",
    "        else:\n",
    "            time_idx = accel_time\n",
    "\n",
    "        # 2) Flatten time and apply Time2Vec => shape: (B*T, 1) -> (B*T, time2vec_dim)\n",
    "        time_flat = time_idx.reshape(-1, 1)\n",
    "        t_emb_flat = self.time2vec(time_flat)\n",
    "        t_emb = t_emb_flat.view(B, T, self.time2vec_dim)\n",
    "\n",
    "        # 3) Concatenate accelerometer data and Time2Vec embedding => (B, T, 3+8=11)\n",
    "        x = torch.cat([accel_seq, t_emb], dim=-1)\n",
    "\n",
    "        # 4) Project to d_model dimension => (B, T, d_model)\n",
    "        x_proj = self.input_proj(x)\n",
    "\n",
    "        # 5) Process through Transformer encoder => (B, T, d_model)\n",
    "        out = self.encoder(x_proj, src_key_padding_mask=accel_mask)\n",
    "\n",
    "        # 6) Global average pool across time => (B, d_model)\n",
    "        out = out.mean(dim=1)\n",
    "\n",
    "        # 7) Final linear layer => (B, num_classes)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "os.environ['PJRT_DEVICE'] = 'CPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b68c05-d56a-44b6-bc81-c95a7d2dc07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpn5qvbumu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpn5qvbumu/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model exported as fall_time2vec_transformer.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1739345238.951233    1726 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739345238.951447    1726 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-12 01:27:18.953104: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpn5qvbumu\n",
      "2025-02-12 01:27:18.954464: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-12 01:27:18.954491: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpn5qvbumu\n",
      "I0000 00:00:1739345238.962546    1726 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2025-02-12 01:27:18.963641: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-12 01:27:19.038930: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpn5qvbumu\n",
      "2025-02-12 01:27:19.053206: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 100110 microseconds.\n",
      "2025-02-12 01:27:19.108919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-12 01:27:19.370023: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 1.389 M  ops, equivalently 0.695 M  MACs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import ai_edge_torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Disable Torch XLA to force CUDA usage.\n",
    "os.environ[\"USE_TORCH_XLA\"] = \"0\"\n",
    "\n",
    "# Updated fallback implementation for scaled dot product attention.\n",
    "def fallback_scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    if attn_mask is not None:\n",
    "        # Ensure that the attention mask is a boolean tensor.\n",
    "        if attn_mask.dtype != torch.bool:\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "        scores = scores.masked_fill(attn_mask, float(\"-inf\"))\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    if dropout_p > 0.0:\n",
    "        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "    return torch.matmul(attn, v)\n",
    "\n",
    "# Override the efficient attention op with our fallback.\n",
    "F.scaled_dot_product_attention = fallback_scaled_dot_product_attention\n",
    "\n",
    "def convert_to_tflite():\n",
    "    # Use CUDA if available.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. Import and instantiate your model, move to CUDA, and set to evaluation mode.\n",
    "    from __main__ import FallTime2VecTransformer  # Ensure model definition cell is run\n",
    "    model = FallTime2VecTransformer().to(device).eval()\n",
    "\n",
    "    # 2. Prepare sample inputs:\n",
    "    #    - accel_seq: (B, T, 3) accelerometer inertial data (x, y, z)\n",
    "    #    - accel_mask: (B, T) boolean mask (here all zeros, no padding)\n",
    "    #    - accel_time: Provide a dummy tensor instead of None.\n",
    "    B, T, C = 1, 10, 3  # Example: batch=1, 10 time steps, 3 channels\n",
    "    accel_seq = torch.randn(B, T, C, dtype=torch.float32, device=device)\n",
    "    accel_mask = torch.zeros(B, T, dtype=torch.bool, device=device)\n",
    "    # Create a dummy accel_time tensor, e.g., a range for each time step.\n",
    "    accel_time = torch.arange(T, device=device).unsqueeze(0).expand(B, T).float()\n",
    "\n",
    "    sample_inputs = (accel_seq, accel_mask, accel_time)\n",
    "\n",
    "    # 3. Convert the model using AI Edge Torch.\n",
    "    #    This produces a single-signature, float32 TFLite model.\n",
    "    edge_model = ai_edge_torch.convert(model, sample_inputs)\n",
    "\n",
    "    # 4. Export the converted model as a TFLite flatbuffer file.\n",
    "    edge_model.export(\"fall_time2vec_transformer.tflite\")\n",
    "    print(\"TFLite model exported as fall_time2vec_transformer.tflite\")\n",
    "\n",
    "# Run the conversion function.\n",
    "convert_to_tflite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d53877f-4614-4524-a7c0-6f2e9ee00136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Inspecting PyTorch Model *****\n",
      "=== PyTorch Model Architecture ===\n",
      "FallTime2VecTransformer(\n",
      "  (time2vec): Time2Vec()\n",
      "  (input_proj): Linear(in_features=11, out_features=64, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total number of parameters: 67,858\n",
      "Estimated memory footprint (float32): 0.26 MB\n",
      "\n",
      "--- Parameter Details ---\n",
      "time2vec.lin_weight                      | shape: (1,)            | dtype: torch.float32\n",
      "time2vec.lin_bias                        | shape: (1,)            | dtype: torch.float32\n",
      "time2vec.per_weight                      | shape: (7,)            | dtype: torch.float32\n",
      "time2vec.per_bias                        | shape: (7,)            | dtype: torch.float32\n",
      "input_proj.weight                        | shape: (64, 11)        | dtype: torch.float32\n",
      "input_proj.bias                          | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.self_attn.in_proj_weight | shape: (192, 64)       | dtype: torch.float32\n",
      "encoder.layers.0.self_attn.in_proj_bias  | shape: (192,)          | dtype: torch.float32\n",
      "encoder.layers.0.self_attn.out_proj.weight | shape: (64, 64)        | dtype: torch.float32\n",
      "encoder.layers.0.self_attn.out_proj.bias | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.linear1.weight          | shape: (128, 64)       | dtype: torch.float32\n",
      "encoder.layers.0.linear1.bias            | shape: (128,)          | dtype: torch.float32\n",
      "encoder.layers.0.linear2.weight          | shape: (64, 128)       | dtype: torch.float32\n",
      "encoder.layers.0.linear2.bias            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.norm1.weight            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.norm1.bias              | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.norm2.weight            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.0.norm2.bias              | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.self_attn.in_proj_weight | shape: (192, 64)       | dtype: torch.float32\n",
      "encoder.layers.1.self_attn.in_proj_bias  | shape: (192,)          | dtype: torch.float32\n",
      "encoder.layers.1.self_attn.out_proj.weight | shape: (64, 64)        | dtype: torch.float32\n",
      "encoder.layers.1.self_attn.out_proj.bias | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.linear1.weight          | shape: (128, 64)       | dtype: torch.float32\n",
      "encoder.layers.1.linear1.bias            | shape: (128,)          | dtype: torch.float32\n",
      "encoder.layers.1.linear2.weight          | shape: (64, 128)       | dtype: torch.float32\n",
      "encoder.layers.1.linear2.bias            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.norm1.weight            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.norm1.bias              | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.norm2.weight            | shape: (64,)           | dtype: torch.float32\n",
      "encoder.layers.1.norm2.bias              | shape: (64,)           | dtype: torch.float32\n",
      "fc.weight                                | shape: (2, 64)         | dtype: torch.float32\n",
      "fc.bias                                  | shape: (2,)            | dtype: torch.float32\n",
      "\n",
      "***** Inspecting TFLite Model *****\n",
      "\n",
      "=== TFLite Model Details ===\n",
      "TFLite model file size: 163.29 KB\n",
      "\n",
      "--- Input Tensor Details ---\n",
      "Name: serving_default_args_2:0, shape: [ 1 10], dtype: <class 'numpy.float32'>\n",
      "Name: serving_default_args_0:0, shape: [ 1 10  3], dtype: <class 'numpy.float32'>\n",
      "Name: serving_default_args_1:0, shape: [ 1 10], dtype: <class 'numpy.bool'>\n",
      "\n",
      "--- Output Tensor Details ---\n",
      "Name: StatefulPartitionedCall:0, shape: [1 2], dtype: <class 'numpy.float32'>\n",
      "\n",
      "--- All TFLite Tensor Details ---\n",
      "Name: serving_default_args_2:0, shape: [ 1 10], dtype: <class 'numpy.float32'>\n",
      "Name: serving_default_args_0:0, shape: [ 1 10  3], dtype: <class 'numpy.float32'>\n",
      "Name: serving_default_args_1:0, shape: [ 1 10], dtype: <class 'numpy.bool'>\n",
      "Name: arith.constant, shape: [ 2 64], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant1, shape: [2], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant2, shape: [ 64 128], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant3, shape: [128  64], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant4, shape: [64 64], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant5, shape: [192  64], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant6, shape: [64 11], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant7, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant8, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant9, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant10, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant11, shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant12, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant13, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant14, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant15, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant16, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant17, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant18, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant19, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant20, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant21, shape: [5], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant22, shape: [5], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant23, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant24, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant25, shape: [1], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant26, shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant27, shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant28, shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant29, shape: [], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant30, shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: arith.constant31, shape: [], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant32, shape: [], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant33, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant34, shape: [ 1 10], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant35, shape: [ 1 10], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant36, shape: [ 1  4  1 10], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant37, shape: [64], dtype: <class 'numpy.float32'>\n",
      "Name: arith.constant38, shape: [128], dtype: <class 'numpy.float32'>\n",
      "Name: XlaCallModule/ReadVariableOp_22;StatefulPartitionedCall, shape: [64], dtype: <class 'numpy.float32'>\n",
      "Name: XlaCallModule/ReadVariableOp_24;StatefulPartitionedCall, shape: [7], dtype: <class 'numpy.float32'>\n",
      "Name: XlaCallModule/ReadVariableOp_25;StatefulPartitionedCall, shape: [7], dtype: <class 'numpy.float32'>\n",
      "Name: XlaCallModule/ReadVariableOp_26;StatefulPartitionedCall, shape: [1], dtype: <class 'numpy.float32'>\n",
      "Name: XlaCallModule/ReadVariableOp_27;StatefulPartitionedCall, shape: [1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer;, shape: [10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;, shape: [10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;1, shape: [10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;2, shape: [10  7], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;3, shape: [10  7], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;4, shape: [10  7], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/__main__.Time2Vec_time2vec;5, shape: [10  8], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer;1, shape: [ 1 10  8], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer;2, shape: [ 1 10 11], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.linear.Linear_input_proj;, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;, shape: [ 1 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;1, shape: [ 1  1  1 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;2, shape: [ 1  4  1 10], dtype: <class 'numpy.bool'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;3, shape: [ 10 192], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;4, shape: [ 1 10  1  3 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;5, shape: [ 3 10  1  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;6, shape: [ 3 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;7, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;8, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;9, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;10, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;11, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;12, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;13, shape: [ 1  4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;14, shape: [ 1  4 16 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;15, shape: [ 4 16 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;16, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;17, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;18, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;19, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;20, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;21, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;22, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;23, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;24, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;25, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;26, shape: [ 1  4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;27, shape: [10  1  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;28, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;29, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;;__main__.FallTime2VecTransformer/torch.nn.modules.linear.Linear_input_proj;, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;;__main__.FallTime2VecTransformer/torch.nn.modules.linear.Linear_input_proj;1, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;30, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;31, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;32, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;33, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;34, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;35, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;36, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;37, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;38, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;39, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;40, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;41, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;42, shape: [ 10 128], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;43, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;44, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;45, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;46, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;47, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;48, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;49, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;50, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;51, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;52, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;53, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;54, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;55, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;56, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;57, shape: [  1  10 192], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;58, shape: [ 1 10  1  3 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;59, shape: [ 3 10  1  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;60, shape: [ 3 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;61, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;62, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;63, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;64, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;65, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;66, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;67, shape: [ 1  4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;68, shape: [ 1  4 16 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;69, shape: [ 4 16 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;70, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;71, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;72, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;73, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;74, shape: [ 1  4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;75, shape: [ 4 10 10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;76, shape: [ 1 10  1 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;77, shape: [10  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;78, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;79, shape: [ 4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;80, shape: [ 1  4 10 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;81, shape: [10  1  4 16], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;82, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;83, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;84, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;85, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;86, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;87, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;88, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;89, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;90, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;91, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;92, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;93, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;94, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;95, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;96, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;97, shape: [ 10 128], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;98, shape: [10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;99, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;100, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;101, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;102, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;103, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;104, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;105, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;106, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;107, shape: [10], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;108, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;109, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;110, shape: [ 1 10  1], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer/torch.nn.modules.transformer.TransformerEncoder_encoder;111, shape: [ 1 10 64], dtype: <class 'numpy.float32'>\n",
      "Name: __main__.FallTime2VecTransformer;3, shape: [ 1 64], dtype: <class 'numpy.float32'>\n",
      "Name: StatefulPartitionedCall:0, shape: [1 2], dtype: <class 'numpy.float32'>\n",
      "Name: , shape: [3], dtype: <class 'numpy.int32'>\n",
      "Name: , shape: [1], dtype: <class 'numpy.int32'>\n",
      "Name: , shape: [64], dtype: <class 'numpy.float32'>\n",
      "\n",
      "***** Comparison Summary *****\n",
      "PyTorch model was converted preserving float32 precision.\n",
      "All parameter dtypes in the PyTorch model are expected to be torch.float32.\n",
      "TFLite model input and output dtypes should be np.float32 (as seen above).\n",
      "Differences in internal tensor names and organization are expected due to the conversion process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Detailed Comparison of PyTorch and TFLite Models\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 1. Inspect the original PyTorch model.\n",
    "def inspect_pytorch_model():\n",
    "    model = FallTime2VecTransformer().eval()\n",
    "    print(\"=== PyTorch Model Architecture ===\")\n",
    "    print(model)\n",
    "    \n",
    "    # Count total parameters.\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"\\nTotal number of parameters: {:,}\".format(total_params))\n",
    "    \n",
    "    # Assuming all parameters are float32 (4 bytes each).\n",
    "    mem_footprint_bytes = total_params * 4\n",
    "    print(\"Estimated memory footprint (float32): {:.2f} MB\".format(mem_footprint_bytes / (1024 ** 2)))\n",
    "    \n",
    "    # List each parameter (name, shape, dtype)\n",
    "    print(\"\\n--- Parameter Details ---\")\n",
    "    for name, param in model.named_parameters():\n",
    "        shape_str = str(tuple(param.shape))\n",
    "        print(f\"{name:40s} | shape: {shape_str:15s} | dtype: {param.dtype}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "# 2. Inspect the TFLite model.\n",
    "def inspect_tflite_model(tflite_file):\n",
    "    print(\"\\n=== TFLite Model Details ===\")\n",
    "    # Check file size.\n",
    "    file_size = os.path.getsize(tflite_file)\n",
    "    print(f\"TFLite model file size: {file_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Load the TFLite model.\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_file)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Input details.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    print(\"\\n--- Input Tensor Details ---\")\n",
    "    for inp in input_details:\n",
    "        print(f\"Name: {inp['name']}, shape: {inp['shape']}, dtype: {inp['dtype']}\")\n",
    "    \n",
    "    # Output details.\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print(\"\\n--- Output Tensor Details ---\")\n",
    "    for out in output_details:\n",
    "        print(f\"Name: {out['name']}, shape: {out['shape']}, dtype: {out['dtype']}\")\n",
    "    \n",
    "    # Get all tensor details.\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    print(\"\\n--- All TFLite Tensor Details ---\")\n",
    "    for tensor in tensor_details:\n",
    "        print(f\"Name: {tensor.get('name', 'N/A')}, shape: {tensor['shape']}, dtype: {tensor['dtype']}\")\n",
    "    \n",
    "    return interpreter\n",
    "\n",
    "# 3. Run comparisons.\n",
    "print(\"***** Inspecting PyTorch Model *****\")\n",
    "pt_model = inspect_pytorch_model()\n",
    "\n",
    "tflite_filename = \"fall_time2vec_transformer.tflite\"\n",
    "if os.path.exists(tflite_filename):\n",
    "    print(\"\\n***** Inspecting TFLite Model *****\")\n",
    "    interpreter = inspect_tflite_model(tflite_filename)\n",
    "else:\n",
    "    print(f\"\\nTFLite file '{tflite_filename}' not found. Run the conversion script first.\")\n",
    "\n",
    "# 4. Summary of Precision and Comparison\n",
    "print(\"\\n***** Comparison Summary *****\")\n",
    "print(\"PyTorch model was converted preserving float32 precision.\")\n",
    "print(\"All parameter dtypes in the PyTorch model are expected to be torch.float32.\")\n",
    "print(\"TFLite model input and output dtypes should be np.float32 (as seen above).\")\n",
    "print(\"Differences in internal tensor names and organization are expected due to the conversion process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4738c6-9d72-4eb0-8937-009418eb6542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Edge Torch)",
   "language": "python",
   "name": "ai_edgetorch_convert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
