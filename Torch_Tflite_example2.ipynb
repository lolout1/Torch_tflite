{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948e7649-932a-452e-9f6b-4f1eff6da897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:23:17.728622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732159397.776086  275183 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732159397.794692  275183 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:23:17.897936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_275183/1112376939.py:207: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732159415.398424  275183 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptujrlrz3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptujrlrz3/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732159418.625831  275183 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732159418.625903  275183 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:23:38.627858: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmptujrlrz3\n",
      "2024-11-20 21:23:38.629511: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:23:38.629543: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmptujrlrz3\n",
      "I0000 00:00:1732159418.644897  275183 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:23:38.647251: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:23:38.751646: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmptujrlrz3\n",
      "2024-11-20 21:23:38.781135: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 153299 microseconds.\n",
      "2024-11-20 21:23:38.818596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-20 21:23:39.107914: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.395 M  ops, equivalently 5.698 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Convert the wrapped model to LiteRT\n",
    "    try:\n",
    "        edge_model = ai_edge_torch.convert(wrapped_model, sample_args)\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b734794f-549f-4ecf-8e19-e5a3b2b1b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported to 'mobile_falldet.tflite'\n"
     ]
    }
   ],
   "source": [
    "# Export the LiteRT model to TFLite\n",
    "edge_model.export('mobile_falldet2.tflite')\n",
    "print(\"Model successfully exported to 'mobile_falldet.tflite'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a27e13-63e0-4147-a40e-1f3f950c5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:10:47.494061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732158647.519374  271538 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732158647.527195  271538 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:10:47.569229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_271538/735383770.py:207: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n",
      "W1120 21:10:51.581000 271538 torch/_export/__init__.py:64] +============================+\n",
      "W1120 21:10:51.582000 271538 torch/_export/__init__.py:65] |     !!!   WARNING   !!!    |\n",
      "W1120 21:10:51.584000 271538 torch/_export/__init__.py:66] +============================+\n",
      "W1120 21:10:51.585000 271538 torch/_export/__init__.py:67] capture_pre_autograd_graph() is deprecated and doesn't provide any function guarantee moving forward.\n",
      "W1120 21:10:51.587000 271538 torch/_export/__init__.py:68] Please switch to use torch.export.export_for_training instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n",
      "I0000 00:00:1732158665.497585  271538 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt9ri5xs0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt9ri5xs0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion failed: Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True<unknown>:0: error: loc(callsite(callsite(callsite(\"torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl/__main__.DualPathFallDetector_original_model/__main__.SMVProcessor_phone_smv_processor/torch.nn.modules.container.Sequential_smv_encoder/torch.nn.modules.conv.Conv1d_0;\" at fused[\"XlaCallModule:\", \"XlaCallModule@__inference_inner_572\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_731\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tfl.transpose' op has mismatched quantized axes of input and output\n",
      "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n",
      "\n",
      "Failed to save quantized model: name 'pt2e_drq_model' is not defined\n",
      "Failed to export quantized model to TFLite: name 'pt2e_drq_model' is not defined\n",
      "Validation of quantized model failed: name 'pt2e_drq_model' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732158668.828573  271538 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732158668.828642  271538 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:11:08.829464: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpt9ri5xs0\n",
      "2024-11-20 21:11:08.831443: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:11:08.831467: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpt9ri5xs0\n",
      "I0000 00:00:1732158668.850352  271538 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:11:08.853217: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:11:09.004122: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpt9ri5xs0\n",
      "2024-11-20 21:11:09.039545: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 210085 microseconds.\n",
      "2024-11-20 21:11:09.070943: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "loc(callsite(callsite(callsite(\"torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl/__main__.DualPathFallDetector_original_model/__main__.SMVProcessor_phone_smv_processor/torch.nn.modules.container.Sequential_smv_encoder/torch.nn.modules.conv.Conv1d_0;\" at fused[\"XlaCallModule:\", \"XlaCallModule@__inference_inner_572\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_731\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tfl.transpose' op has mismatched quantized axes of input and output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Quantization Steps\n",
    "    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
    "    from ai_edge_torch.quantize.quant_config import QuantConfig\n",
    "\n",
    "    # Initialize the PT2E Quantizer with symmetric quantization configuration\n",
    "    pt2e_quantizer = PT2EQuantizer().set_global(\n",
    "        get_symmetric_quantization_config(is_per_channel=True, is_dynamic=True)\n",
    "    )\n",
    "\n",
    "    # Capture the pre-autograd graph of the wrapped model\n",
    "    pt2e_torch_model = capture_pre_autograd_graph(wrapped_model, sample_args)\n",
    "\n",
    "    # Prepare the model for PT2E quantization\n",
    "    pt2e_torch_model = prepare_pt2e(pt2e_torch_model, pt2e_quantizer)\n",
    "\n",
    "    # Run the prepared model with sample input data to ensure that internal observers are populated with correct values\n",
    "    pt2e_torch_model(*sample_args)\n",
    "\n",
    "    # Convert the prepared model to a quantized model\n",
    "    pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
    "\n",
    "    # Convert to an ai_edge_torch model with quantization configuration and additional converter flags\n",
    "    try:\n",
    "        pt2e_drq_model = ai_edge_torch.convert(\n",
    "            pt2e_torch_model,\n",
    "            sample_args,\n",
    "            quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer),\n",
    "            _ai_edge_converter_flags={'experimental_enable_resource_variables': True}\n",
    "        )\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "        # Depending on the error, consider alternative approaches below\n",
    "\n",
    "    # Save the quantized model with a different name\n",
    "    try:\n",
    "        quantized_model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414_quant.pth\"\n",
    "        torch.save(pt2e_drq_model.state_dict(), quantized_model_path)\n",
    "        print(f\"Quantized model saved successfully at '{quantized_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save quantized model: {e}\")\n",
    "\n",
    "    # Optional: Convert the quantized model to TFLite\n",
    "    try:\n",
    "        # Export the quantized LiteRT model to TFLite\n",
    "        pt2e_drq_model.export('mobile_falldet_quant.tflite')\n",
    "        print(\"Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export quantized model to TFLite: {e}\")\n",
    "\n",
    "    # Optional: Validate the Quantized Model\n",
    "    try:\n",
    "        # Perform inference with the quantized PyTorch model\n",
    "        with torch.no_grad():\n",
    "            torch_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Perform inference with the quantized LiteRT model\n",
    "        # Assuming that pt2e_drq_model can perform inference like this\n",
    "        # If not, you may need to load the TFLite model separately for inference\n",
    "        tfl_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Extract logits and SMV features\n",
    "        torch_quant_logits = torch_quant_output[0].detach().numpy()\n",
    "        tfl_quant_logits = tfl_quant_output[0]\n",
    "\n",
    "        torch_quant_smv_features = {k: v.detach().numpy() for k, v in torch_quant_output[1].items()}\n",
    "        tfl_quant_smv_features = {k: v for k, v in tfl_quant_output[1].items()}\n",
    "\n",
    "        # Compare logits\n",
    "        if np.allclose(torch_quant_logits, tfl_quant_logits, atol=1e-5, rtol=1e-5):\n",
    "            print(\"Quantized inference result for logits with PyTorch and LiteRT matches within tolerance.\")\n",
    "        else:\n",
    "            print(\"Discrepancy found in quantized logits between PyTorch and LiteRT models.\")\n",
    "\n",
    "        # Compare SMV features\n",
    "        for key in torch_quant_smv_features:\n",
    "            if np.allclose(torch_quant_smv_features[key], tfl_quant_smv_features[key], atol=1e-5, rtol=1e-5):\n",
    "                print(f\"Quantized inference result for {key} matches within tolerance.\")\n",
    "            else:\n",
    "                print(f\"Discrepancy found in quantized {key} between PyTorch and LiteRT models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation of quantized model failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f62090-2e52-4a8f-993a-176d3aa4d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/abheekp/kd/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/abheekp/kd/lib/python3.10/site-packages (0.20.1)\n",
      "Collecting ai-edge-torch\n",
      "  Downloading ai_edge_torch-0.2.1-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==3.1.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: networkx in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: fsspec in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: filelock in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/abheekp/kd/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/abheekp/kd/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/abheekp/kd/lib/python3.10/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/abheekp/kd/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Collecting ai-edge-quantizer-nightly==0.0.1.dev20240718\n",
      "  Using cached ai_edge_quantizer_nightly-0.0.1.dev20240718-py3-none-any.whl (100 kB)\n",
      "Collecting torch-xla<2.6,>=2.4.0\n",
      "  Downloading torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl (90.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tf-nightly>=2.18.0.dev20240722\n",
      "  Downloading tf_nightly-2.19.0.dev20241119-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (631.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.2/631.2 MB\u001b[0m \u001b[31m425.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m33.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/abheekp/kd/lib/python3.10/site-packages (from ai-edge-torch) (1.10.1)\n",
      "Collecting safetensors\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Collecting immutabledict\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: packaging in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Collecting h5py>=3.11.0\n",
      "  Using cached h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Collecting keras>=3.5.0\n",
      "  Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/abheekp/kd/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.3/381.3 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abheekp/kd/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Collecting keras-nightly>=3.6.0.dev\n",
      "  Downloading keras_nightly-3.7.0.dev2024112003-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tb-nightly~=2.19.0.a\n",
      "  Downloading tb_nightly-2.19.0a20241120-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/abheekp/kd/lib/python3.10/site-packages (from torch-xla<2.6,>=2.4.0->ai-edge-torch) (6.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abheekp/kd/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/abheekp/kd/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, tabulate, safetensors, optree, opt-einsum, numpy, mdurl, markdown, immutabledict, grpcio, google-pasta, gast, absl-py, torch-xla, tensorboard, tb-nightly, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras-nightly, keras, tf-nightly, tensorflow, ai-edge-quantizer-nightly, ai-edge-torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed absl-py-2.1.0 ai-edge-quantizer-nightly-0.0.1.dev20240718 ai-edge-torch-0.2.1 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 h5py-3.12.1 immutabledict-4.2.1 keras-3.6.0 keras-nightly-3.7.0.dev2024112003 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.13.1 rich-13.9.4 safetensors-0.4.5 tabulate-0.9.0 tb-nightly-2.19.0a20241120 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tf-nightly-2.19.0.dev20241119 torch-xla-2.5.1 werkzeug-3.1.3 wheel-0.45.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision ai-edge-torch tensorflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd74f4b6-5e9d-4306-9a34-d2fcaaaf9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_1) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_2) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_3) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_4) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_5) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_6) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/abheekp/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/torch/fx/graph.py:1062: UserWarning: erase_node(_native_batch_norm_legit_no_training_7) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rx_dv7s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0rx_dv7s/assets\n",
      "W0000 00:00:1732158995.136254  271538 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732158995.136791  271538 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:16:35.138567: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.140593: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:16:35.140633: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.156286: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:16:35.275884: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp0rx_dv7s\n",
      "2024-11-20 21:16:35.299945: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 161527 microseconds.\n",
      "2024-11-20 21:16:35.755475: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.297 M  ops, equivalently 5.648 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n",
      "Failed to save quantized model: 'TfLiteModel' object has no attribute 'state_dict'\n",
      "Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\n",
      "Validation of quantized model failed: 'numpy.ndarray' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "\n",
    "# Suppress specific UserWarnings from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Can be set to False if needed\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Quantization Steps\n",
    "    from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
    "    from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
    "    from ai_edge_torch.quantize.quant_config import QuantConfig\n",
    "\n",
    "    # Initialize the PT2E Quantizer with symmetric quantization configuration (per-tensor and dynamic)\n",
    "    pt2e_quantizer = PT2EQuantizer().set_global(\n",
    "        get_symmetric_quantization_config(is_per_channel=False, is_dynamic=True)\n",
    "    )\n",
    "\n",
    "    # Capture the pre-autograd graph of the wrapped model\n",
    "    pt2e_torch_model = capture_pre_autograd_graph(wrapped_model, sample_args)\n",
    "\n",
    "    # Prepare the model for PT2E quantization\n",
    "    pt2e_torch_model = prepare_pt2e(pt2e_torch_model, pt2e_quantizer)\n",
    "\n",
    "    # Run the prepared model with sample input data to ensure that internal observers are populated with correct values\n",
    "    pt2e_torch_model(*sample_args)\n",
    "\n",
    "    # Convert the prepared model to a quantized model\n",
    "    pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
    "\n",
    "    # Conversion Flags\n",
    "    _ai_edge_converter_flags = {\n",
    "        'experimental_enable_resource_variables': True\n",
    "    }\n",
    "\n",
    "    # Convert to an ai_edge_torch model with quantization configuration and additional converter flags\n",
    "    try:\n",
    "        pt2e_drq_model = ai_edge_torch.convert(\n",
    "            pt2e_torch_model,\n",
    "            sample_args,\n",
    "            quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer),\n",
    "            _ai_edge_converter_flags=_ai_edge_converter_flags\n",
    "        )\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "        # Depending on the error, consider alternative approaches below\n",
    "\n",
    "    # Save the quantized model with a different name\n",
    "    try:\n",
    "        quantized_model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414_quant.pth\"\n",
    "        torch.save(pt2e_drq_model.state_dict(), quantized_model_path)\n",
    "        print(f\"Quantized model saved successfully at '{quantized_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save quantized model: {e}\")\n",
    "\n",
    "    # Optional: Convert the quantized model to TFLite\n",
    "    try:\n",
    "        # Export the quantized LiteRT model to TFLite\n",
    "        pt2e_drq_model.export('mobile_falldet_quant.tflite')\n",
    "        print(\"Quantized model successfully exported to 'mobile_falldet_quant.tflite'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export quantized model to TFLite: {e}\")\n",
    "\n",
    "    # Optional: Validate the Quantized Model\n",
    "    try:\n",
    "        # Perform inference with the quantized PyTorch model\n",
    "        with torch.no_grad():\n",
    "            torch_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Perform inference with the quantized LiteRT model\n",
    "        # Note: AI Edge Torch's LiteRT model might require a different inference approach.\n",
    "        # Here, we assume it can be invoked similarly.\n",
    "        # If not, you may need to load the TFLite model separately for inference.\n",
    "        tfl_quant_output = pt2e_drq_model(*sample_args)\n",
    "\n",
    "        # Extract logits and SMV features\n",
    "        torch_quant_logits = torch_quant_output[0].detach().numpy()\n",
    "        tfl_quant_logits = tfl_quant_output[0]\n",
    "\n",
    "        torch_quant_smv_features = {k: v.detach().numpy() for k, v in torch_quant_output[1].items()}\n",
    "        tfl_quant_smv_features = {k: v for k, v in tfl_quant_output[1].items()}\n",
    "\n",
    "        # Compare logits\n",
    "        if np.allclose(torch_quant_logits, tfl_quant_logits, atol=1e-5, rtol=1e-5):\n",
    "            print(\"Quantized inference result for logits with PyTorch and LiteRT matches within tolerance.\")\n",
    "        else:\n",
    "            print(\"Discrepancy found in quantized logits between PyTorch and LiteRT models.\")\n",
    "\n",
    "        # Compare SMV features\n",
    "        for key in torch_quant_smv_features:\n",
    "            if np.allclose(torch_quant_smv_features[key], tfl_quant_smv_features[key], atol=1e-5, rtol=1e-5):\n",
    "                print(f\"Quantized inference result for {key} matches within tolerance.\")\n",
    "            else:\n",
    "                print(f\"Discrepancy found in quantized {key} between PyTorch and LiteRT models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation of quantized model failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416ffc0e-1f66-4eb0-b3ee-273f8940a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details:\n",
      "{'name': 'serving_default_args_0:0', 'index': 0, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'serving_default_args_1:0', 'index': 1, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Output Details:\n",
      "{'name': 'StatefulPartitionedCall:1', 'index': 407, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 398, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:2', 'index': 414, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Logits: [0.42323497]\n",
      "SMV Features: [[-2.19007    1.9581431]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "`dim` must be in the range [-1, 1) where 1 is the number of dimensions in the input. Received: dim=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMV Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, smv_features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Post-processing: Apply softmax to logits to get probabilities\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProbabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, probabilities)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Determine the predicted class\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/AI_EDGETORCH_CONVERT/lib/python3.10/site-packages/tensorflow/python/ops/nn_ops.py:3836\u001b[0m, in \u001b[0;36m_wrap_2d_function\u001b[0;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[1;32m   3834\u001b[0m   dim_val \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconstant_value(dim)\n\u001b[1;32m   3835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39mshape\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dim_val \u001b[38;5;241m<\u001b[39m shape\u001b[38;5;241m.\u001b[39mndims:\n\u001b[0;32m-> 3836\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m   3837\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3838\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dim` must be in the range [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m-\u001b[39mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) where \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3839\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;241m.\u001b[39mndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is the number of dimensions in the input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3840\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;66;03m# If dim is not the last dimension, we have to do a transpose so that we can\u001b[39;00m\n\u001b[1;32m   3843\u001b[0m \u001b[38;5;66;03m# still perform the op on its last dimension.\u001b[39;00m\n\u001b[1;32m   3844\u001b[0m \n\u001b[1;32m   3845\u001b[0m \u001b[38;5;66;03m# In case dim is negative (and is not last dimension -1), add shape.ndims\u001b[39;00m\n\u001b[1;32m   3846\u001b[0m ndims \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mrank(inputs)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: `dim` must be in the range [-1, 1) where 1 is the number of dimensions in the input. Received: dim=1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your TFLite model\n",
    "tflite_model_path = \"mobile_falldet_quant.tflite\"\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\")\n",
    "for detail in input_details:\n",
    "    print(detail)\n",
    "\n",
    "print(\"\\nOutput Details:\")\n",
    "for detail in output_details:\n",
    "    print(detail)\n",
    "\n",
    "# Prepare sample input data\n",
    "# Replace these with actual sensor data in practice\n",
    "phone_input = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "# Set tensor for 'accelerometer_phone'\n",
    "interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "\n",
    "# Set tensor for 'accelerometer_watch'\n",
    "interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensors\n",
    "logits = interpreter.get_tensor(output_details[0]['index'])\n",
    "smv_features = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "print(\"\\nLogits:\", logits)\n",
    "print(\"SMV Features:\", smv_features)\n",
    "\n",
    "# Post-processing: Apply softmax to logits to get probabilities\n",
    "probabilities = tf.nn.softmax(logits, axis=1).numpy()\n",
    "print(\"\\nProbabilities:\", probabilities)\n",
    "\n",
    "# Determine the predicted class\n",
    "predicted_class = np.argmax(probabilities, axis=1)\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edec4e2f-067b-47cc-89f6-8b72e82d06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details:\n",
      "{'name': 'serving_default_args_0:0', 'index': 0, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'serving_default_args_1:0', 'index': 1, 'shape': array([  1, 128,   4], dtype=int32), 'shape_signature': array([  1, 128,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "Output Details:\n",
      "{'name': 'StatefulPartitionedCall:1', 'index': 466, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 460, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'StatefulPartitionedCall:2', 'index': 472, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Processing Output Tensor: StatefulPartitionedCall:1, Shape: [1], Data: [0.01611325]\n",
      "Assigned 'StatefulPartitionedCall:1' to smv_features_1.\n",
      "Processing Output Tensor: StatefulPartitionedCall:0, Shape: [1 2], Data: [[ 1.5169027 -1.52473  ]]\n",
      "Assigned 'StatefulPartitionedCall:0' to logits.\n",
      "Processing Output Tensor: StatefulPartitionedCall:2, Shape: [1], Data: [0.01524284]\n",
      "Assigned 'StatefulPartitionedCall:2' to smv_features_2.\n",
      "\n",
      "Logits: [[ 1.5169027 -1.52473  ]]\n",
      "SMV Features 1: [0.01611325]\n",
      "SMV Features 2: [0.01524284]\n",
      "\n",
      "Probabilities: [[0.95441985 0.04558009]]\n",
      "Predicted Class: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Path to your TFLite model\n",
    "tflite_model_path = \"mobile_falldet2.tflite\"\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\")\n",
    "for detail in input_details:\n",
    "    print(detail)\n",
    "\n",
    "print(\"\\nOutput Details:\")\n",
    "for detail in output_details:\n",
    "    print(detail)\n",
    "\n",
    "# Prepare sample input data\n",
    "# Replace these with actual sensor data in practice\n",
    "phone_input = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "# Set tensor for 'accelerometer_phone'\n",
    "interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "\n",
    "# Set tensor for 'accelerometer_watch'\n",
    "interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Assign outputs based on name and shape\n",
    "logits = None\n",
    "smv_features_1 = None\n",
    "smv_features_2 = None\n",
    "\n",
    "for detail in output_details:\n",
    "    tensor = interpreter.get_tensor(detail['index'])\n",
    "    name = detail['name']\n",
    "    shape = detail['shape']\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(f\"Processing Output Tensor: {name}, Shape: {shape}, Data: {tensor}\")\n",
    "\n",
    "    # Correct shape comparison using np.array_equal\n",
    "    if name == 'StatefulPartitionedCall:0' and np.array_equal(shape, [1, 2]):\n",
    "        logits = tensor\n",
    "        print(f\"Assigned '{name}' to logits.\")\n",
    "    elif name == 'StatefulPartitionedCall:1' and np.array_equal(shape, [1]):\n",
    "        smv_features_1 = tensor\n",
    "        print(f\"Assigned '{name}' to smv_features_1.\")\n",
    "    elif name == 'StatefulPartitionedCall:2' and np.array_equal(shape, [1]):\n",
    "        smv_features_2 = tensor\n",
    "        print(f\"Assigned '{name}' to smv_features_2.\")\n",
    "    else:\n",
    "        print(f\"Unrecognized tensor: {name} with shape: {shape}\")\n",
    "\n",
    "# Verify that logits have been correctly assigned\n",
    "if logits is None:\n",
    "    raise ValueError(\"Logits tensor not found in the model outputs.\")\n",
    "if smv_features_1 is None or smv_features_2 is None:\n",
    "    print(\"Warning: One or more SMV features were not found in the model outputs.\")\n",
    "\n",
    "print(\"\\nLogits:\", logits)\n",
    "print(\"SMV Features 1:\", smv_features_1)\n",
    "print(\"SMV Features 2:\", smv_features_2)\n",
    "\n",
    "# Post-processing: Apply softmax to logits to get probabilities\n",
    "# Ensure that logits have shape [1, 2] before applying softmax\n",
    "if logits.ndim == 2 and logits.shape[1] == 2:\n",
    "    probabilities = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    print(\"\\nProbabilities:\", probabilities)\n",
    "    \n",
    "    # Determine the predicted class\n",
    "    predicted_class = np.argmax(probabilities, axis=1)\n",
    "    print(\"Predicted Class:\", predicted_class)\n",
    "else:\n",
    "    print(\"Logits tensor has an unexpected shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac264bd-fbbf-4f77-a529-85f767901ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:30:06.809417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732159806.835889  277252 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732159806.844495  277252 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 21:30:06.892289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "/tmp/ipykernel_277252/4071810581.py:216: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  add_safe_globals([np.core.multiarray.scalar])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load checkpoint with weights_only=True: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Attempting to load without weights_only=True (security risk)...\n",
      "Checkpoint loaded successfully with weights_only=False.\n",
      "State_dict loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732159823.564281  277252 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpay1u275n/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpay1u275n/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1732159826.296060  277252 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1732159826.296149  277252 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-20 21:30:26.296846: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpay1u275n\n",
      "2024-11-20 21:30:26.298441: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-20 21:30:26.298464: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpay1u275n\n",
      "I0000 00:00:1732159826.314439  277252 mlir_graph_optimization_pass.cc:402] MLIR V1 optimization pass is not enabled\n",
      "2024-11-20 21:30:26.316565: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-20 21:30:26.414157: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpay1u275n\n",
      "2024-11-20 21:30:26.441781: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 144938 microseconds.\n",
      "2024-11-20 21:30:26.469479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-20 21:30:26.740856: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 11.395 M  ops, equivalently 5.698 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model conversion successful!\n",
      "TFLite model saved successfully at 'mobile_falldet2.tflite'.\n",
      "\n",
      "PyTorch Model Outputs:\n",
      "Logits: [[ 1.4914067 -1.5174068]]\n",
      "SMV Features: {'phone_smv': array([0.01581427], dtype=float32), 'watch_smv': array([0.01431987], dtype=float32)}\n",
      "Processing Output Tensor: StatefulPartitionedCall:1, Shape: [1], Data: [0.01581427]\n",
      "Assigned 'StatefulPartitionedCall:1' to smv_features_1.\n",
      "Processing Output Tensor: StatefulPartitionedCall:0, Shape: [1 2], Data: [[ 1.4914067 -1.517407 ]]\n",
      "Assigned 'StatefulPartitionedCall:0' to logits.\n",
      "Processing Output Tensor: StatefulPartitionedCall:2, Shape: [1], Data: [0.01431986]\n",
      "Assigned 'StatefulPartitionedCall:2' to smv_features_2.\n",
      "\n",
      "TFLite Model Outputs:\n",
      "Logits: [[ 1.4914067 -1.517407 ]]\n",
      "SMV Features: {'smv_features_1': array([0.01581427], dtype=float32), 'smv_features_2': array([0.01431986], dtype=float32)}\n",
      "\n",
      "Logits Comparison:\n",
      "Absolute Differences: [[0.0000000e+00 1.1920929e-07]]\n",
      "Mean Squared Error (MSE): 7.105427357601002e-15\n",
      "Mean Absolute Error (MAE): 5.960464477539063e-08\n",
      "\n",
      "SMV Feature 'phone_smv' not found in TFLite outputs.\n",
      "\n",
      "SMV Feature 'watch_smv' not found in TFLite outputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress the specific UserWarning from the Transformer module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.transformer\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define the PyTorch Model\n",
    "# ------------------------------\n",
    "\n",
    "# Define the XYZProcessor\n",
    "class XYZProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.xyz_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.xyz_encoder(x)\n",
    "\n",
    "# Define the SMVProcessor\n",
    "class SMVProcessor(nn.Module):\n",
    "    def __init__(self, hidden_dim, sequence_length, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.smv_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.threshold_learner = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.smv_encoder(x)\n",
    "        threshold = self.threshold_learner(features)\n",
    "        return features, threshold\n",
    "\n",
    "# Define the DualPathFallDetector\n",
    "class DualPathFallDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        acc_coords=4,\n",
    "        sequence_length=128,\n",
    "        hidden_dim=64,\n",
    "        num_heads=8,\n",
    "        depth=4,\n",
    "        mlp_ratio=4,\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        use_skeleton=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Processors\n",
    "        self.phone_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.phone_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        self.watch_xyz_processor = XYZProcessor(hidden_dim, dropout)\n",
    "        self.watch_smv_processor = SMVProcessor(hidden_dim, sequence_length, dropout)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # You can set this to False if nested tensors are required\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth,\n",
    "            norm=nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def process_device_data(self, data):\n",
    "        \"\"\"Process data from one device with SMV calculation\"\"\"\n",
    "        # Split XYZ and calculate SMV\n",
    "        xyz_data = data[:, :, :3]  # [B, T, 3]\n",
    "        smv_data = torch.norm(xyz_data, dim=2, keepdim=True)  # [B, T, 1]\n",
    "        \n",
    "        # Process XYZ coordinates\n",
    "        xyz_data = rearrange(xyz_data, 'b t c -> b c t')\n",
    "        xyz_features = self.phone_xyz_processor(xyz_data)  # [B, H, T/2]\n",
    "        xyz_features = xyz_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Process SMV signal\n",
    "        smv_data = rearrange(smv_data, 'b t c -> b c t')\n",
    "        smv_features, smv_threshold = self.phone_smv_processor(smv_data)\n",
    "        smv_features = smv_features.mean(dim=2)  # [B, H]\n",
    "        \n",
    "        # Combine features\n",
    "        device_features = torch.cat([xyz_features, smv_features], dim=1)  # [B, 2H]\n",
    "        \n",
    "        return device_features, smv_threshold\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass with both classification and SMV features\"\"\"\n",
    "        # Process phone data\n",
    "        phone_features, phone_threshold = self.process_device_data(\n",
    "            data['accelerometer_phone'].float()\n",
    "        )\n",
    "        \n",
    "        # Process watch data\n",
    "        watch_features, watch_threshold = self.process_device_data(\n",
    "            data['accelerometer_watch'].float()\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([phone_features, watch_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        temporal = fused.unsqueeze(1)\n",
    "        temporal = self.transformer(temporal)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = temporal.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        # Return both logits and SMV features\n",
    "        smv_features = {\n",
    "            'phone_smv': phone_threshold.squeeze(-1),\n",
    "            'watch_smv': watch_threshold.squeeze(-1),\n",
    "        }\n",
    "        \n",
    "        return logits, smv_features\n",
    "\n",
    "# Define the Wrapper Module\n",
    "class DualPathFallDetectorWrapper(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DualPathFallDetectorWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, accelerometer_phone, accelerometer_watch):\n",
    "        data = {\n",
    "            'accelerometer_phone': accelerometer_phone,\n",
    "            'accelerometer_watch': accelerometer_watch\n",
    "        }\n",
    "        logits, smv_features = self.original_model(data)\n",
    "        return logits, smv_features\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load and Convert the PyTorch Model\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize the model\n",
    "model = DualPathFallDetector(\n",
    "    acc_coords=4,\n",
    "    sequence_length=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=8,\n",
    "    depth=4,\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    use_skeleton=False\n",
    ")\n",
    "\n",
    "# Path to your checkpoint\n",
    "model_path = \"exps/smartfall_har/mobile_falldet/model_epoch_22_f1_0.9414.pth\"  # Use forward slashes for cross-platform compatibility\n",
    "\n",
    "# Option 1: Add safe globals and load checkpoint with weights_only=True\n",
    "try:\n",
    "    add_safe_globals([np.core.multiarray.scalar])\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint loaded successfully with weights_only=True.\")\n",
    "except AttributeError:\n",
    "    print(\"add_safe_globals is not available in your PyTorch version. Please update PyTorch to >=2.1.0.\")\n",
    "    checkpoint = None\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint with weights_only=True: {e}\")\n",
    "    print(\"Attempting to load without weights_only=True (security risk)...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Checkpoint loaded successfully with weights_only=False.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load checkpoint with weights_only=False: {e2}\")\n",
    "        checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    # Load state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"State_dict loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State_dict loaded directly from checkpoint.\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Wrap the model\n",
    "    wrapped_model = DualPathFallDetectorWrapper(model).eval()\n",
    "\n",
    "    # Prepare sample inputs as a tuple of tensors\n",
    "    batch_size = 1\n",
    "    sequence_length = 128\n",
    "    channels_phone = 4  # Adjust based on your data\n",
    "    channels_watch = 4  # Adjust based on your data\n",
    "\n",
    "    sample_args = (\n",
    "        torch.randn(batch_size, sequence_length, channels_phone),\n",
    "        torch.randn(batch_size, sequence_length, channels_watch)\n",
    "    )\n",
    "\n",
    "    # Set PJRT_DEVICE to 'CPU' to address the CUDA-related RuntimeError\n",
    "    os.environ['PJRT_DEVICE'] = 'CPU'\n",
    "\n",
    "    # Convert the wrapped model to LiteRT\n",
    "    try:\n",
    "        edge_model = ai_edge_torch.convert(wrapped_model, sample_args)\n",
    "        print(\"Model conversion successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model conversion failed: {e}\")\n",
    "else:\n",
    "    print(\"Checkpoint loading failed. Conversion cannot proceed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Save the TFLite Model\n",
    "# ------------------------------\n",
    "\n",
    "# Assuming the conversion was successful, save the TFLite model\n",
    "if 'edge_model' in locals():\n",
    "    try:\n",
    "        tflite_model_path = \"mobile_falldet2.tflite\"\n",
    "        edge_model.export(tflite_model_path)\n",
    "        print(f\"TFLite model saved successfully at '{tflite_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save TFLite model: {e}\")\n",
    "else:\n",
    "    print(\"Edge model not found. Cannot export to TFLite.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Perform Inference and Compare Outputs\n",
    "# ------------------------------\n",
    "\n",
    "# Function to run PyTorch inference\n",
    "def run_pytorch_inference(model, phone_input, watch_input):\n",
    "    \"\"\"\n",
    "    Runs inference on the PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The wrapped PyTorch model.\n",
    "        phone_input (torch.Tensor): Tensor for accelerometer_phone.\n",
    "        watch_input (torch.Tensor): Tensor for accelerometer_watch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: logits as NumPy array and SMV features as a dictionary.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, smv_features = model(phone_input, watch_input)\n",
    "    return logits.numpy(), {k: v.numpy() for k, v in smv_features.items()}\n",
    "\n",
    "# Function to run TFLite inference\n",
    "def run_tflite_inference(tflite_model_path, phone_input, watch_input):\n",
    "    \"\"\"\n",
    "    Runs inference on the TFLite model.\n",
    "\n",
    "    Args:\n",
    "        tflite_model_path (str): Path to the TFLite model.\n",
    "        phone_input (np.ndarray): Input array for accelerometer_phone.\n",
    "        watch_input (np.ndarray): Input array for accelerometer_watch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: logits as NumPy array and SMV features as a dictionary.\n",
    "    \"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensor details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Ensure the inputs are in the correct order\n",
    "    # Assuming 'serving_default_args_0:0' corresponds to 'accelerometer_phone'\n",
    "    # and 'serving_default_args_1:0' corresponds to 'accelerometer_watch'\n",
    "    interpreter.set_tensor(input_details[0]['index'], phone_input)\n",
    "    interpreter.set_tensor(input_details[1]['index'], watch_input)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Assign outputs based on name and shape\n",
    "    logits = None\n",
    "    smv_features_1 = None\n",
    "    smv_features_2 = None\n",
    "\n",
    "    for detail in output_details:\n",
    "        tensor = interpreter.get_tensor(detail['index'])\n",
    "        name = detail['name']\n",
    "        shape = detail['shape']\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(f\"Processing Output Tensor: {name}, Shape: {shape}, Data: {tensor}\")\n",
    "\n",
    "        # Correct shape comparison using np.array_equal\n",
    "        if name == 'StatefulPartitionedCall:0' and np.array_equal(shape, [1, 2]):\n",
    "            logits = tensor\n",
    "            print(f\"Assigned '{name}' to logits.\")\n",
    "        elif name == 'StatefulPartitionedCall:1' and np.array_equal(shape, [1]):\n",
    "            smv_features_1 = tensor\n",
    "            print(f\"Assigned '{name}' to smv_features_1.\")\n",
    "        elif name == 'StatefulPartitionedCall:2' and np.array_equal(shape, [1]):\n",
    "            smv_features_2 = tensor\n",
    "            print(f\"Assigned '{name}' to smv_features_2.\")\n",
    "        else:\n",
    "            print(f\"Unrecognized tensor: {name} with shape: {shape}\")\n",
    "\n",
    "    # Verify that logits have been correctly assigned\n",
    "    if logits is None:\n",
    "        raise ValueError(\"Logits tensor not found in the model outputs.\")\n",
    "    if smv_features_1 is None or smv_features_2 is None:\n",
    "        print(\"Warning: One or more SMV features were not found in the model outputs.\")\n",
    "\n",
    "    return logits, {'smv_features_1': smv_features_1, 'smv_features_2': smv_features_2}\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Generate Consistent Random Input Data\n",
    "# ------------------------------\n",
    "\n",
    "# For reproducibility, set the random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate deterministic random input data\n",
    "phone_input_np = np.random.rand(1, 128, 4).astype(np.float32)    # Shape: [1, 128, 4]\n",
    "watch_input_np = np.random.rand(1, 128, 4).astype(np.float32)   # Shape: [1, 128, 4]\n",
    "\n",
    "phone_input_torch = torch.from_numpy(phone_input_np)\n",
    "watch_input_torch = torch.from_numpy(watch_input_np)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Run Inference on Both Models\n",
    "# ------------------------------\n",
    "\n",
    "# Run PyTorch inference\n",
    "if 'wrapped_model' in locals():\n",
    "    pytorch_logits, pytorch_smv = run_pytorch_inference(wrapped_model, phone_input_torch, watch_input_torch)\n",
    "    print(\"\\nPyTorch Model Outputs:\")\n",
    "    print(\"Logits:\", pytorch_logits)\n",
    "    print(\"SMV Features:\", pytorch_smv)\n",
    "else:\n",
    "    print(\"Wrapped model not found. Skipping PyTorch inference.\")\n",
    "\n",
    "# Run TFLite inference\n",
    "if os.path.exists(\"mobile_falldet2.tflite\"):\n",
    "    tflite_logits, tflite_smv = run_tflite_inference(\"mobile_falldet2.tflite\", phone_input_np, watch_input_np)\n",
    "    print(\"\\nTFLite Model Outputs:\")\n",
    "    print(\"Logits:\", tflite_logits)\n",
    "    print(\"SMV Features:\", tflite_smv)\n",
    "else:\n",
    "    print(\"TFLite model file 'mobile_falldet2.tflite' not found. Skipping TFLite inference.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Compare the Outputs\n",
    "# ------------------------------\n",
    "\n",
    "if 'pytorch_logits' in locals() and 'tflite_logits' in locals():\n",
    "    # Compare logits\n",
    "    logits_diff = np.abs(pytorch_logits - tflite_logits)\n",
    "    logits_mse = np.mean((pytorch_logits - tflite_logits) ** 2)\n",
    "    logits_mae = np.mean(logits_diff)\n",
    "    \n",
    "    print(\"\\nLogits Comparison:\")\n",
    "    print(\"Absolute Differences:\", logits_diff)\n",
    "    print(f\"Mean Squared Error (MSE): {logits_mse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {logits_mae}\")\n",
    "\n",
    "    # Compare SMV features\n",
    "    for key in pytorch_smv:\n",
    "        pytorch_smv_feat = pytorch_smv[key]\n",
    "        tflite_smv_feat = tflite_smv.get(key, None)\n",
    "        \n",
    "        if tflite_smv_feat is not None:\n",
    "            smv_diff = np.abs(pytorch_smv_feat - tflite_smv_feat)\n",
    "            smv_mse = np.mean((pytorch_smv_feat - tflite_smv_feat) ** 2)\n",
    "            smv_mae = np.mean(smv_diff)\n",
    "            \n",
    "            print(f\"\\nSMV Feature '{key}' Comparison:\")\n",
    "            print(\"Absolute Differences:\", smv_diff)\n",
    "            print(f\"Mean Squared Error (MSE): {smv_mse}\")\n",
    "            print(f\"Mean Absolute Error (MAE): {smv_mae}\")\n",
    "        else:\n",
    "            print(f\"\\nSMV Feature '{key}' not found in TFLite outputs.\")\n",
    "else:\n",
    "    print(\"Insufficient outputs to perform comparison.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Edge Torch)",
   "language": "python",
   "name": "ai_edgetorch_convert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
