{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PJRT_DEVICE\"] = \"CPU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abheekp/ai_edge_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_36651/846237639.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pth_file_path, map_location='cpu')\n",
      "I0000 00:00:1730397274.947128   36651 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjo2y6ts0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjo2y6ts0/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1730397276.881676   36651 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1730397276.881733   36651 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-10-31 12:54:36.882774: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpjo2y6ts0\n",
      "2024-10-31 12:54:36.883570: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-31 12:54:36.883584: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpjo2y6ts0\n",
      "I0000 00:00:1730397276.893503   36651 mlir_graph_optimization_pass.cc:360] MLIR V1 optimization pass is not enabled\n",
      "2024-10-31 12:54:36.894925: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-31 12:54:36.977234: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpjo2y6ts0\n",
      "2024-10-31 12:54:36.995705: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 112928 microseconds.\n",
      "2024-10-31 12:54:37.050575: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-31 12:54:37.311350: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 13.082 M  ops, equivalently 6.541 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted to TFLite format and saved as 'transmodel_converted1.tflite'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict, Tuple\n",
    "from torch.nn import Linear, LayerNorm, TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math\n",
    "import ai_edge_torch  # Ensure AI Edge Torch is installed\n",
    "\n",
    "\n",
    "class TransformerEncoderWAttention(nn.TransformerEncoder):\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "        # Make sure there are no mutable state changes by not storing attention weights\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "class TransModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 mocap_frames=128,\n",
    "                 num_joints=32,\n",
    "                 acc_frames=128,\n",
    "                 num_classes=2,     \n",
    "                 num_heads=2,\n",
    "                 acc_coords=4,\n",
    "                 av=False,\n",
    "                 num_layer=2,\n",
    "                 norm_first=True,\n",
    "                 embed_dim=32,     \n",
    "                 activation='relu',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.data_shape = (acc_frames, acc_coords)\n",
    "        self.length = self.data_shape[0]  # 128\n",
    "        size = self.data_shape[1]         # 4\n",
    "\n",
    "       \n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Conv1d(size, embed_dim, kernel_size=3, stride=1, padding='same'),          \n",
    "            nn.Conv1d(embed_dim, embed_dim * 2, kernel_size=3, stride=1, padding='same'), \n",
    "            nn.Conv1d(embed_dim * 2, embed_dim, kernel_size=3, stride=1, padding='same')  \n",
    "        )\n",
    "\n",
    "        \n",
    "        self.encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=self.length,          # 128\n",
    "            nhead=num_heads,              # 2\n",
    "            dim_feedforward=32,\n",
    "            activation=activation,\n",
    "            dropout=0.5,\n",
    "            batch_first=False,\n",
    "            norm_first=norm_first\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.encoder = TransformerEncoderWAttention(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layer,\n",
    "            norm=nn.LayerNorm(embed_dim)  # LayerNorm over embed_dim (32)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.ln1 = nn.Linear(self.length, 32)  # 128 -> 32\n",
    "        self.ln2 = nn.Linear(32, 16)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.output = Linear(16, num_classes)  # 16 -> 2\n",
    "        nn.init.normal_(self.output.weight, 0, math.sqrt(2. / num_classes))\n",
    "\n",
    "    def forward(self, acc_data, skl_data):\n",
    "        b, l, c = acc_data.shape  # b, 128, 4\n",
    "        x = rearrange(acc_data, 'b l c -> b c l')  # [b, 4, 128]\n",
    "        x = self.input_proj(x)                     # [b, 32, 128]\n",
    "        x = rearrange(x, 'b c l -> c b l')         # [32, b, 128]\n",
    "        x = self.encoder(x)                        # [32, b, 128]\n",
    "        x = rearrange(x, 'c b l -> b l c')         # [b, 128, 32]\n",
    "\n",
    "        \n",
    "        x = F.avg_pool1d(x, kernel_size=x.shape[-1], stride=1)  # [b, 128, 1]\n",
    "        x = x.view(b, -1)                          # [b, 128]\n",
    "        x = F.relu(self.ln1(x))                    # [b, 32]\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.ln2(x))                    # [b, 16]\n",
    "        x = self.output(x)                         # [b, num_classes]\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_and_convert_model(pth_file_path, tflite_file_path):\n",
    "    \n",
    "    model = TransModel()\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    checkpoint = torch.load(pth_file_path, map_location='cpu')\n",
    "\n",
    "    \n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "    \n",
    "    class ModelWrapper(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, acc_data):\n",
    "            skl_data_dummy = torch.zeros(acc_data.shape[0], 128, 32, 3)\n",
    "            return self.model(acc_data, skl_data_dummy)\n",
    "\n",
    "    wrapped_model = ModelWrapper()\n",
    "    wrapped_model.eval()\n",
    "    wrapped_model.to('cpu')\n",
    "\n",
    "    \n",
    "    acc_data_sample = torch.randn(1, 128, 4)\n",
    "\n",
    "   \n",
    "   \n",
    "    edge_model = ai_edge_torch.convert(\n",
    "        wrapped_model,\n",
    "        (acc_data_sample,)\n",
    "    )\n",
    "\n",
    "    \n",
    "    edge_model.export(tflite_file_path)\n",
    "    print(f\"Model successfully converted to TFLite format and saved as '{tflite_file_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your saved .pth file\n",
    "    pth_file_path = \"ttfWeights.pth\"  \n",
    "    tflite_file_path = \"transmodel_converted1.tflite\"\n",
    "\n",
    "    \n",
    "    load_and_convert_model(pth_file_path, tflite_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: serving_default_args_0:0\n",
      "Shape: [  1 128   4]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant\n",
      "Shape: [2]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant1\n",
      "Shape: [ 32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant2\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant3\n",
      "Shape: [384 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant4\n",
      "Shape: [384]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant5\n",
      "Shape: [ 32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant6\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant7\n",
      "Shape: [384 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant8\n",
      "Shape: [384]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant9\n",
      "Shape: [ 2 16]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant10\n",
      "Shape: [16 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant11\n",
      "Shape: [ 32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant12\n",
      "Shape: [128  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant13\n",
      "Shape: [128 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant14\n",
      "Shape: [128  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant15\n",
      "Shape: [128 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant16\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant17\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant18\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant19\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant20\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant21\n",
      "Shape: [64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant22\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant23\n",
      "Shape: [ 1 16]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant24\n",
      "Shape: [ 1 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant25\n",
      "Shape: [32  1  3 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant26\n",
      "Shape: [64  1  3 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant27\n",
      "Shape: [32  1  3  4]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant28\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant29\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant30\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant31\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant32\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant33\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant34\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant35\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant36\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant37\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant38\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant39\n",
      "Shape: [5]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant40\n",
      "Shape: [5]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant41\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant42\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant43\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant44\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant45\n",
      "Shape: [3]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant46\n",
      "Shape: []\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant47\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant48\n",
      "Shape: [4]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant49\n",
      "Shape: [2]\n",
      "Data type: <class 'numpy.int32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant50\n",
      "Shape: []\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: arith.constant51\n",
      "Shape: []\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: XlaCallModule/ReadVariableOp_11;StatefulPartitionedCall\n",
      "Shape: [128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: XlaCallModule/ReadVariableOp_16;StatefulPartitionedCall\n",
      "Shape: [128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: XlaCallModule/ReadVariableOp_21;StatefulPartitionedCall\n",
      "Shape: [128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: XlaCallModule/ReadVariableOp_6;StatefulPartitionedCall\n",
      "Shape: [128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;\n",
      "Shape: [  1   4 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_0;\n",
      "Shape: [  1   4   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_0;1\n",
      "Shape: [  1   1 128   4]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_0;2\n",
      "Shape: [  1   1 128  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_1;\n",
      "Shape: [  1   1 128  64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_2;\n",
      "Shape: [  1   1 128  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.container.Sequential_input_proj/torch.nn.modules.conv.Conv1d_2;1\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;1\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;1\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;2\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;3\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;4\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;5\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;6\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;7\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;8\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;9\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;10\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;11\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;12\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;;XlaCallModule/ReadVariableOp_25;StatefulPartitionedCall;XlaCallModule/ReadVariableOp_32\n",
      "Shape: [  1  32 384]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;13\n",
      "Shape: [  1  32   1   3 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;14\n",
      "Shape: [  3  32   1   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;15\n",
      "Shape: [  3  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;16\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;17\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;18\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;19\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;20\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;21\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;22\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;23\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;24\n",
      "Shape: [ 1  2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;25\n",
      "Shape: [ 1  2 64 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;26\n",
      "Shape: [ 2 64 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;27\n",
      "Shape: [ 2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;28\n",
      "Shape: [ 1  2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;29\n",
      "Shape: [ 1  2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;30\n",
      "Shape: [ 2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;31\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;32\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;33\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;34\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;35\n",
      "Shape: [ 1  2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;36\n",
      "Shape: [32  1  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;37\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;38\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;39\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;40\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;41\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;42\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;43\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;44\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;45\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;46\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;47\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;48\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;49\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;50\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;51\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;52\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;;XlaCallModule/ReadVariableOp_20;StatefulPartitionedCall;XlaCallModule/ReadVariableOp_33\n",
      "Shape: [32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;53\n",
      "Shape: [ 32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;54\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_0;55\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;1\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;2\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;3\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;4\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;5\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;6\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;7\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;8\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;9\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;10\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;11\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;12\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;;XlaCallModule/ReadVariableOp_15;StatefulPartitionedCall;XlaCallModule/ReadVariableOp_34\n",
      "Shape: [  1  32 384]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;13\n",
      "Shape: [  1  32   1   3 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;14\n",
      "Shape: [  3  32   1   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;15\n",
      "Shape: [  3  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;16\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;17\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;18\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;19\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;20\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;21\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;22\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;23\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;24\n",
      "Shape: [ 1  2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;25\n",
      "Shape: [ 1  2 64 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;26\n",
      "Shape: [ 2 64 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;27\n",
      "Shape: [ 2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;28\n",
      "Shape: [ 1  2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;29\n",
      "Shape: [ 1  2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;30\n",
      "Shape: [ 2 32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;31\n",
      "Shape: [  1  32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;32\n",
      "Shape: [32  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;33\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;34\n",
      "Shape: [ 2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;35\n",
      "Shape: [ 1  2 32 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;36\n",
      "Shape: [32  1  2 64]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;37\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;38\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;39\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;40\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;41\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;42\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;43\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;44\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;45\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;46\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;47\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;48\n",
      "Shape: [32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;49\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;50\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;51\n",
      "Shape: [ 1 32  1]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;52\n",
      "Shape: [  1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;;XlaCallModule/ReadVariableOp_10;StatefulPartitionedCall;XlaCallModule/ReadVariableOp_35\n",
      "Shape: [32 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;53\n",
      "Shape: [ 32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;54\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/__main__.TransformerEncoderWAttention_encoder/torch.nn.modules.transformer.TransformerEncoderLayer_1;55\n",
      "Shape: [ 32   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;2\n",
      "Shape: [  1 128  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;3\n",
      "Shape: [  1 128   1  32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;4\n",
      "Shape: [  1   1  32 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;5\n",
      "Shape: [  1   1   1 128]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.linear.Linear_ln1;;__main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;\n",
      "Shape: [ 1 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.dropout.Dropout_drop2;;__main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.linear.Linear_ln1;\n",
      "Shape: [ 1 32]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.linear.Linear_ln2;\n",
      "Shape: [ 1 16]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: __main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model;;__main__.load_and_convert_model.<locals>.ModelWrapper/__main__.TransModel_model/torch.nn.modules.linear.Linear_ln2;\n",
      "Shape: [ 1 16]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n",
      "Layer name: StatefulPartitionedCall:0\n",
      "Shape: [1 2]\n",
      "Data type: <class 'numpy.float32'>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"transmodel_converted1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get details of all layers\n",
    "for layer in interpreter.get_tensor_details():\n",
    "    print(\"Layer name:\", layer['name'])\n",
    "    print(\"Shape:\", layer['shape'])\n",
    "    print(\"Data type:\", layer['dtype'])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransModel(\n",
       "  (input_proj): Sequential(\n",
       "    (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoderWAttention(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ln1): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (ln2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (output): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    model = TransModel()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Model Structure with Intermediate Outputs:\n",
      "============================================================\n",
      "Layer Type: Conv1d\n",
      "  Weight shape: (32, 4, 3)\n",
      "  Bias shape: (32,)\n",
      "  Input shape: (1, 4, 128)\n",
      "  Output shape: (1, 32, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Conv1d\n",
      "  Weight shape: (64, 32, 3)\n",
      "  Bias shape: (64,)\n",
      "  Input shape: (1, 32, 128)\n",
      "  Output shape: (1, 64, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Conv1d\n",
      "  Weight shape: (32, 64, 3)\n",
      "  Bias shape: (32,)\n",
      "  Input shape: (1, 64, 128)\n",
      "  Output shape: (1, 32, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: LayerNorm\n",
      "  Weight shape: (128,)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: LayerNorm\n",
      "  Weight shape: (128,)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (32, 128)\n",
      "  Bias shape: (32,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 32)\n",
      "  Output shape: (32, 1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (128, 32)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 32)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: LayerNorm\n",
      "  Weight shape: (128,)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: LayerNorm\n",
      "  Weight shape: (128,)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (32, 128)\n",
      "  Bias shape: (32,)\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 32)\n",
      "  Output shape: (32, 1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (128, 32)\n",
      "  Bias shape: (128,)\n",
      "  Input shape: (32, 1, 32)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (32, 1, 128)\n",
      "  Output shape: (32, 1, 128)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (32, 128)\n",
      "  Bias shape: (32,)\n",
      "  Input shape: (1, 128)\n",
      "  Output shape: (1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Dropout\n",
      "  Input shape: (1, 32)\n",
      "  Output shape: (1, 32)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (16, 32)\n",
      "  Bias shape: (16,)\n",
      "  Input shape: (1, 32)\n",
      "  Output shape: (1, 16)\n",
      "------------------------------------------------------------\n",
      "Layer Type: Linear\n",
      "  Weight shape: (2, 16)\n",
      "  Bias shape: (2,)\n",
      "  Input shape: (1, 16)\n",
      "  Output shape: (1, 2)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_detailed_model_layers(model, input_sample):\n",
    "    print(\"Detailed Model Structure with Intermediate Outputs:\\n\" + \"=\" * 60)\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        layer_name = module.__class__.__name__\n",
    "        print(f\"Layer Type: {layer_name}\")\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            print(f\"  Weight shape: {tuple(module.weight.shape)}\")\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            print(f\"  Bias shape: {tuple(module.bias.shape)}\")\n",
    "        print(f\"  Input shape: {tuple(input[0].shape)}\")\n",
    "        print(f\"  Output shape: {tuple(output.shape)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    hooks = []\n",
    "    for layer in model.modules():\n",
    "        if len(list(layer.children())) == 0:  # Only leaf layers\n",
    "            hooks.append(layer.register_forward_hook(forward_hook))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(input_sample, torch.zeros(input_sample.shape[0], 128, 32, 3))  # Sample forward pass\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    model = TransModel()\n",
    "\n",
    "   \n",
    "    input_sample = torch.randn(1, 128, 4) \n",
    "\n",
    "  \n",
    "    print_detailed_model_layers(model, input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_edge_env)",
   "language": "python",
   "name": "ai_edge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
